<h1 style="color:#1a49b5"><center>Exploration approfondie des bases de donn√©es NoSQL : Une plong√©e dans le monde de MongoDB et Redis</center></h1>
<hr>

<h2>Table des mati√®res:</h2>
<h4>Introduction</h4>
<h4>I. la D√©normalisation</h4>
1. √âtude de cas<br>
2. La tranformation des fichiers<br>
3. La jointure sur des objets JSON<br>
<h4>II. D√©couverte de la base de donn√©e Redis</h4>
1. Installation de Redis<br>
2. Connecter √† Redis avec Python<br>
3. Les commandes fondammentales<br>
4. Bloom Filter<br>
5. √âtude des performances de Redis<br>
6. Publication et souscription<br>
<h4>III. D√©couverte de la base de donn√©e MongoDB</h4>
1. Installation de MongoDB<br>
2. Commandes fondamentales de MongoDB<br>
3. Les commandes fondammentales<br>
4. La jointure des Objets JSON avec MongoDB<br>
5. √âtude des performances de MongoDB<br>
6. Publication et souscription<br>

<h4>Conclusion</h4>
<hr>

<h3>Introduction:</h3>

<p style="text-align: justify">
Les syst√®mes de gestion de base de donn√©es traditionnels ont longtemps domin√© le paysage informatique, mais avec l'√©volution rapide des applications modernes, de nouvelles approches sont n√©cessaires pour r√©pondre aux exigences de performances et de flexibilit√© croissantes. Parmi ces approches novatrices, les bases de donn√©es NoSQL √©mergent comme des solutions puissantes, d√©passant les limites des mod√®les relationnels classiques. Dans ce contexte, notre compte rendu se penchera sur deux acteurs majeurs du monde NoSQL : MongoDB et Redis.
</p>
<p style="text-align: justify">
Notre exploration approfondie des bases de donn√©es NoSQL, avec une attention particuli√®re port√©e sur MongoDB et Redis, se centrera sur un √©l√©ment cl√© de leur architecture : la d√©normalisation des tables. Avant d'approfondir ce concept essentiel, nous examinerons de pr√®s les raisons et les crit√®res sous-tendant la d√©normalisation. Cette d√©marche pr√©liminaire jettera les bases n√©cessaires pour une compr√©hension approfondie des avantages et des implications de cette approche dans le contexte de MongoDB et Redis.

La prochaine √©tape nous conduira √† explorer en d√©tail les commandes de base de Redis, mettant en exergue une m√©thode tout particuli√®rement fascinante : le filtre de Bloom. Cette approche s'av√®re √™tre une m√©thode efficace pour optimiser les op√©rations de stockage et de recherche, ajoutant une dimension intrigante √† notre exploration de cette base de donn√©es en m√©moire.

La route de notre exploration se poursuivra avec une √©valuation minutieuse de la performance de Redis, mettant l'accent sur son comportement face √† des ensembles de donn√©es d√©passant les 100 000 √©l√©ments. Cette analyse approfondie jettera une lumi√®re pr√©cise sur la capacit√© de Redis √† √©voluer de mani√®re agile et performante dans des contextes de traitement de donn√©es massives.

Nous plongerons ensuite dans l'univers riche de MongoDB, explorant ses commandes fondamentales et scrutant sa performance dans divers contextes d'utilisation. Cette incursion dans la base de donn√©es orient√©e document nous permettra de saisir les nuances de son approche flexible ax√©e sur la gestion de donn√©es non structur√©es.

Enfin, notre √©tude atteindra son apog√©e avec une comparaison de la performance entre Redis et MongoDB. Cette analyse comparative soulignera les avantages et les sp√©cificit√©s de chaque solution, offrant une vision √©clair√©e pour ceux qui recherchent des solutions NoSQL adapt√©es √† leurs besoins sp√©cifiques. Pr√©parez-vous √† d√©couvrir comment MongoDB et Redis, en combinant souplesse et performance, tracent la voie vers l'avenir de la gestion des informations, r√©pondant ainsi aux d√©fis complexes pos√©s par les applications modernes.
</p>
<hr>

<h2>I. La d√©normalisation:</h2>
<p style="text-align: justify"> 
La d√©normalisation est une technique de conception de bases de donn√©es qui consiste √† regrouper des tables relationnelles initialement normalis√©es pour former une structure plus plate et moins normalis√©e. Cette approche vise √† am√©liorer les performances en r√©duisant le nombre de jointures n√©cessaires lors des requetes. Contrairement √† la normalisation, qui cherche √† organiser les donn√©es pour minimiser la redondance.
</p>

<h3>1. √âtude de cas:</h3>
<p style="text-align: justify">
Afin de mieux comprendre les avantages de la d√©normalisation, consid√©rons le mod√®le relationnel pour une compagnie a√©rienne pr√©sent√© dans <B>la figure 1</B>, Cette d√©marche nous permettre de saisir les raisons sous-jacentes √† l'utilisation de la d√©normalisation et de comprendre son lien avec les bases de donn√©es NoSQL.
</p>
<p style="text-align: justify">
Imaginons une situation ou la compagnie a√©rienne souhaite am√©liorer la performance lors de l'affichage des d√©tails d'un vol, y compris les information sur l'avion, le pilote et la reservation associ√©es. Dans ce mod√®le normalis√©, cela impliquerait des op√©ration de jointure complexes.
</p>

<center>
<img
  src="images/image1.png"
  alt="Mod√®le de donn√©es normalis√© pour une companie a√©rienne"
  width="600"
  height="350" 
  style="text-align: center ;border:1px solid black"/>
<h5><b>Figure 1: Mod√®le de donn√©es normalis√© pour une companie a√©rienne<b></h5>
</center>



<h3 style="text-align: justify">
Comment pouvons-nous d√©normaliser notre Base de donn√©es ?
</h3>
<p >
Voici quelques pistes √† appliquer pour d√©normaliser notre base de donn√©es :
</p>

<p style="text-align: justify">
    
***1.Des donn√©es fr√©quemment interrog√©es conjointement:*** <br>
Dans le cas de nos tables, si nous effectuons fr√©quemment des requ√™tes qui n√©cessitent des informations sur les vols, les r√©servations et les classes de mani√®re conjointe, vous pourriez envisager de d√©normaliser en regroupant ces donn√©es dans une seule table, par exemple une table "InfoVols". Ainsi, au lieu de faire des jointures, vous auriez toutes les informations n√©cessaires dans une seule table.
</p>
<p style="text-align: justify">
    
***2.Toutes les donn√©es d‚Äôune entit√© sont ind√©pendantes:*** <br>
Si les donn√©es des vols, des avions, des pilotes, des clients, des d√©finitions de classes et des r√©servations sont relativement ind√©pendantes les unes des autres, nous pourrions envisager une d√©normalisation en regroupant ces donn√©es dans une table "InfoVols", √©vitant ainsi des jointures fr√©quentes.</p>
<p style="text-align: justify">
    
***3.Une association avec des relations 1-n des deux c√¥t√©s:*** <br>
Si vous avez des associations avec des relations 1-n des deux c√¥t√©s, par exemple entre les vols et les r√©servations, vous pourriez envisager une d√©normalisation en int√©grant les r√©servations directement dans la table "InfoVols", repr√©sent√©e par : ‚Äú[{NumCl, Classe, NbPlaces}]‚Äù.
</p>
<p style="text-align: justify;">
    
***4.M√™me taux de mises √† jour:*** <br>
Si les taux de mise √† jour des diff√©rentes entit√©s (vols, avions, pilotes, clients, etc.) sont √©quivalents, vous pourriez envisager une d√©normalisation plus pouss√©e en regroupant toutes les donn√©es dans une table "InfoVols" pr√©sent√© dans <b>la figure 2</b>, simplifiant ainsi la gestion des mises √† jour.
</p>


<center>
<img
  src="images/image2.png"
  alt="Repr√©sentation de la table  d√©normalis√©e"
  width="500"
  height="400" 
  style="text-align: center ;border:1px solid black"/>
<h5><b>Figure 2: Repr√©sentation de la table  d√©normalis√©e<b></h5>
</center>

<h3>R√©sultat:</h3>
<p style="text-align: justify">
L'impl√©mentation concr√®te notre d√©normalisation est expos√©e dans <b>la figure 3</b>, sous forme de repr√©sentation JSON. Cela sugg√®re que la d√©normalisation a √©t√© r√©alis√©e en regroupant les informations n√©cessaires dans une seule structure de donn√©es, √©liminant ainsi la n√©cessit√© de faire des jointures lors de la r√©cup√©ration des d√©tails d'un vol.
</p>
<center>
<img
  src="images/image3.png"
  alt="Repr√©sentation JSON de la d√©normalisation"
  width="600"
  height="400" 
  style="text-align: center"/>
<h5><b>Figure 3: Repr√©sentation JSON de la d√©normalisation<b></h5>
</center>

<p style="text-align: justify">
La d√©normalisation s'inscrit parfaitement dans l'environnement NoSQL. Ce mod√®le denormalis√© offre la flexibilit√© necessaire pour stocker des donn√©es de mani√®re √† optimiser les requetes sans etre limit√© par une structure de sch√©ma rigide.
</p>
<p style="text-align: justify">
Une fois la d√©normalisation r√©alis√©e, une perspective future serait la migration de la base de donn√©es relationnelle vers une base de donn√©es NoSQL. Ce choix d√©pendra du type sp√©cifique de NoSQL envisag√©, car il existe divers mod√®les tels que la base de donn√©e orient√©e document(comme MongoDB), cl√©-valeur(comme Redis),comme nous le discuterans plus en d√©tails ult√©rieurement. Chacun de ces mod√®le NoSQL offre des avantages en fonction des besoins.
</p>

<h3>2. La tranformation des fichiers:</h3>
<h4>2.1. Transformation des fichiers txt aux fichiers csv:</h4>
<p style="text-align: justify">
Dans cette partie, nous aborderons le processus de transformation d'un fichier au format TXT contenant des donn√©es sur les avions vers un format CSV, en vue de l'int√©grer dans la base de donn√©es. Pour ce faire, nous utilisons le langage de programmation Python et la biblioth√®que pandas. 
<p style="text-align: justify">
Le code commence par lire le fichier "Avions.txt" avec la fonction "read_csv" de pandas, en sp√©cifiant que les donn√©es sont s√©par√©es par des tabulations. Ensuite, des libell√©s de colonnes appropri√©s sont d√©finis dans une liste appel√©e "headers", ces libell√©s sont appliqu√©s au DataFrame r√©sultant.
</p>
<p style="text-align: justify">
Le DataFrame modifi√© est ensuite sauvegard√© dans un fichier CSV "AVIONS.csv" dans le r√©pertoire "csv", en utilisant la fonction "to_csv" de pandas. Le param√®tre "index=None" indique de ne pas inclure les indices de lignes dans le fichier CSV r√©sultant.
</p>
<p>
Ce processus de la transformation cruciale pour pr√©parer les donn√©es afin de les int√©grer efficacement dans la base donn√©es.
</p>

***Fichier_text:***


```python
with open("txt/AVIONS.txt", 'r') as file_avion:
    lignes = file_avion.readlines()
    for index, ligne in enumerate(lignes):
        print(ligne)
        if index==2:
            break
        
```

    720	 Boeing 727	150	Marseille
    
    250	Boeing 747	500	Paris
    
    269	Boeing 737	300	Paris
    


***M√©thode_de_transformation:***


```python
import pandas as pd

file_avion = pd.read_csv ('txt/AVIONS.txt', sep='\t')
headers =  ["NumAv", "NomAv", "CapAv", "VilleAv"]
file_avion.columns = headers
file_avion.to_csv ('csv/AVIONS.csv', index=None)
```

***R√©sultat:***


```python
df = pd.read_csv("csv/AVIONS.csv")
df.head(3)
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>NumAv</th>
      <th>NomAv</th>
      <th>CapAv</th>
      <th>VilleAv</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>250</td>
      <td>Boeing 747</td>
      <td>500</td>
      <td>Paris</td>
    </tr>
    <tr>
      <th>1</th>
      <td>269</td>
      <td>Boeing 737</td>
      <td>300</td>
      <td>Paris</td>
    </tr>
    <tr>
      <th>2</th>
      <td>240</td>
      <td>Boeing 737</td>
      <td>300</td>
      <td>Paris</td>
    </tr>
  </tbody>
</table>
</div>



  <h4>2.2. Transformation des fichiers CSV aux fichiers JSON:</h4>

<p style="text-align: justify">Dans la phase de pr√©paration de notre syst√®me pour l'utilisation de bases de donn√©es NoSQL(MongoDB et Redis), une √©tape importante consiste √† convertir notre jeu de donn√©es, initialement stock√© au format CSV, vers un format JSON.
</p>
<p style="text-align: justify">
Cette op√©ration est r√©alis√©e en utilisant la bibiloth√©que <b>pandans</b> en Python. La transformation s'effectue de mani√®re simple et efficace grace √† la m√©thode "to_json" de pandas:
</p>

***M√©thode_de_transformation:***


```python
df.to_json("json/Avions.json", orient='records')
#Affichage JSON
#df = pd.read_json("json/koko.json")
#df.to_string()
```

<p style="text-align: justify">
Dans cette commande "df" repr√©sente le DataFrame pandas qui contenait les donn√©es extraites du fichier CSV. L'argument "json/avions.json" sp√©cifie le chemain et le nom du fichier JSON r√©sultant dans lequel les donn√©es converties seront enregistr√©es. L'rgument orient='records' sp√©cifie que chaque enregistrement du DataFrame est repr√©sent√© comme un objet JSON distinct dans le fichier r√©sultant.
</p>
<p style="text-align: justify">
Cette conversion pr√©pare nos donn√©es pour etre facilement ing√©r√©es pas les bases de donn√©es NonSql. Le format JSON offre une repr√©sentation adapt√©e a ces bases de donn√©es, facilitant ainsi l'insertion, la mise √† jour et la requete de donn√©es coplexes.
</p>
<p style="text-align: justify">
En proc√©dant de la sorte, notre syst√®me est pret √† exploiter aux mieux les fonctionnalit√©s de stockage et de r√©cup√©ration de nos bases de donn√©es(MongoDB et Redis) et ainsi une gestion efficace et performante de nos donn√©es.
</p>

<h3>3. La jointure sur des objets JSON:</h3>

<p style="text-align: justify">
Dans cette partie, nous aborderons une √©tape cruciale de pr√©paration des donn√©es pour les integr√©s dans une base de donn√©s NoSQL. Notre objectif est d'effectuer une jointure entre deux fichiers JSON, en utilisant un attribut commun, suivi d'une fonction d'une op√©ration de d√©normalisation pour simplifier la structure des donn√©es.
</p>

***√âtape 1: chargement des fichiers JSON (fonction facultatif)***<br>
La fonction "readJsonFile", nous permet de charger un fichier JSON √† partir de de son chemin.


```python
def readJsonFile(jsonPath):
    with open(jsonPath) as f:
        jsonified = json.load(f)
        return jsonified
```

***√âtape 2: Jointure des fichiers JSON***<br>
<p>
La fonction <b>jointure</b> prend deux fichiers JSON ainsi qu'un attribut commun. Elle r√©alise une jointure des deux fichiers o√π la valeur de l'attribut commune est identique.
</p>


```python
def jointure(json1, json2, key):
    file=[]
    for lineJson1 in json1:
        for lineJson2 in json2:
            if lineJson1[key]==lineJson2[key]:
                concatenateLines = lineJson1.copy()
                concatenateLines.update(lineJson2)
                file.append(concatenateLines)
    return file   
```

***√âtape 3: La d√©normalisation des donn√©es***<br>
Cette fonction r√©alise la d√©normalisation des donn√©s en regroupant les objets qui partagent la meme valeur pour une cl√© sp√©cifi√©e.


```python
import json
from itertools import groupby

def denormalizeJsonByKey(json, key):
    data=sorted(json, key=lambda x: x[key])
    jsonResult = {}
    for groupKey, group in groupby(data, key=lambda x: x.get(key, '')):
        groupList = list(group)
        size = (len(groupList))
        if size == 1:
            jsonResult[groupKey] = groupList[0]
        else:
            keysList = list(groupList[0].keys())
            result = {}
            for keyDict in keysList:
                if keyDict == key :
                    result[keyDict] = groupKey
                else:
                    result[keyDict] = []
        
            for obj in groupList:
                for keyDict2, value in obj.items():
                    if keyDict2 != key and value not in result[keyDict2]:
                        result[keyDict2].append(value)
            jsonResult[groupKey] = result
        
    return list(jsonResult.values())        
```

***√âtape 4: test***<br>



```python
key = "cle"
json1 = readJsonFile("json/json1.json")
json2 = readJsonFile("json/json2.json")
combined = jointure(json1, json2, key)
denormalize=denormalizeJsonByKey(combined,key)
#print(denormalize)

chemin_fichier_json = "json/new_file.json"
with open(chemin_fichier_json, 'w') as file:
    json.dump(denormalize, file, indent=2)

```

***Conclusion***<p>
    En conclusion, cette approche de jointure et de d√©normalisation constitue une √©tape essentielle pour pr√©parer les donn√©es en vue de leur int√©gration dans une BD NoSQL.
</p>

<h1>II. D√©couverte de la base de donn√©e Redis:</h1>

<p style="text-align: justify">
Dans cette section, nous plongerons dans Redis, un syst√®me de gestion de donn√©es, con√ßu pour stocker, r√©cup√©rer et manipuler diff√©rents types de donn√©es. Il s‚Äôagit d‚Äôune base de donn√©es NoSQL de type paire cl√©/valeur. 
<p style="text-align: justify">
<b>Note importante: </b>Une des principales caract√©ristiques de Redis est de conserver l'int√©gralit√© des donn√©es en RAM. Cela permet d'obtenir d'excellentes performances en √©vitant les acc√®s disques, particuli√®rement co√ªteux. Pour garantir la pr√©servation des donn√©es en cas d'incident, compte tenu de la volatilit√© de la m√©moire vive, Redis propose la fonctionnalit√© de "capturer" l'√©tat de la base dans un fichier. Bien que cette technique ne retienne pas les modifications apport√©es entres deux captures, elle offre √©galement la possibilit√© de les enregistrer, ainsi la restauration de la base en cas de n√©cessite. 
<p style="text-align: justify">
Pr√©parez-vous √† explorer le monde de Redis! On va d√©couvrir ses utilisations, ses fonctionnalit√©s, ses performances et bien s√ªr, son service pub/sub ultra efficace.
</p>
</p>
<p>
Accrochez-vous, c'est parti! üöÄ
</p>
<p>

</p>
<h3>1. Installation de Redis:</h3>
<p style="text-align: justify">
La proc√©dure d√©taill√©e est disponible dans <b>les Annexes 1 </b>et<b> 2</b>. Nous avons choisi d'utiliser Docker pour le d√©ploiement de Redis en raison de sa facilit√© d'utilisation. Pour ce faire, nous avons cr√©e un Dockerfile. De plus Docker Compose qui orchestre le d√©ploiement de notre application.
</p>
<p style="text-align: justify">
Une alternative pour interagir avec Redis est d'utiliser RedisInsight, une interface graphique conviviale simplifie la gestion et la visualisation des donn√©e Redis.
</p>

<h3>2. Connecter √† Redis avec Python:</h3>

<p>Pour interagir avec Redis, la premi√®re √©tape est la connexion:</p>


```python
import redis
client_redis = redis.Redis(host='localhost', port=6379)
```

<h3>3. Les commandes fondammentales:</h3>
<h4>3.1. Stockez et r√©cup√©rez les donn√©es:</h4>
<h5>3.1.1 Une cha√Æne simple:</h5>


```python
#Stockez
client_redis.set('name', 'karam')

#R√©cup√©rez
client_redis.get('name')
```




    b'karam'



<h5>3.1.2 Un dictionnaire:</h5>


```python
#Stockez
client_redis.hset('users', mapping={
    'name': 'karam',
    "surname": 'bekkali',
    "age": 30
})
#R√©cup√©rez
client_redis.hgetall('users')
```




    {b'name': b'karam', b'surname': b'bekkali', b'age': b'30'}



<h5>3.1.3 Un fichier JSON:</h5>


```python
import json

#Stockez
with open ("json/json1.json") as file:
    data=json.load(file)
client_redis.json().set('my_json','$',data)

#R√©cup√©rez
res = client_redis.execute_command('JSON.GET', 'my_json')
element= res[1]
#element= res
print(element)
```

    {'cle': '001', 'lieu': 'Rane 1'}


<p>
<b>La figure 4</b> montre la structure JSON d'un fichier stock√© dans Redis. La repr√©sentation JSON montre les donn√©es organis√©es avec des cl√©s et des valeurs associ√©es. Les objets JSON sont repr√©sent√©s par des accolades, le tableaux par des crochets, et les paires cl√©-valeur indiquent des informations sp√©cifiques. Les donn√©es sont organis√©es de mani√®re √† permettre une r√©cup√©ration efficace en utilisant les cl√©s d√©finies.
</p>

<center>
<img
  src="images/image4.png"
  alt="D√©monstration d'insertion des donn√©es dans la base de donn√©e Redis"
  width="500"
  height="400" 
  style="text-align: center"/>
<h5><b>Figure 4: D√©monstration d'insertion des donn√©es dans la base de donn√©e Redis<b></h5>
</center>



<h3>4. Bloom Filter:</h3>
<p>
Un filtre bloom est une structure probabiliste des donn√©es qui permet de savoir de fa√ßon extr√™mement rapide si un √©l√©ment fait partie d‚Äôun ensemble ou pas. On entend par probabiliste, le fait que le r√©sultat ne peut qu‚Äô√™tre ‚Äúpas compris dans l‚Äôensemble‚Äù ou ‚Äú√©ventuellement compris dans l‚Äôensemble‚Äù. Il ne peut pas fournir un r√©sultat exact tel que ‚Äúoui l‚Äô√©l√©ment fait assur√©ment partie d‚Äôun ensemble‚Äù. Cela signifie que des faux positifs sont possibles mais que des faux n√©gatifs sont impossibles.
</p>

<h4>4.1. Sc√©nario d'application: La d√©tection des adresses IP ind√©sirable:</h4>

<p>L'objectif de cet exemple est d'explorer l'utilisation des filtres de Bloom dans le contexte de la v√©rification des adresses IP par rapport √† une liste noire.</p>
<p>
Dans cette d√©monstration, nous avons utilis√© l'interface en ligne de commande de RedisInsight pour mettre en oeuvre cette approche. <b>La figure 5</b> montre le processus.
</p>
<p>Dans cet exemple, nous avons utilis√© Bloom Filter pour v√©rifier si des adresses IP font partie d'une liste noire. Tout d'abord, nous avons cr√©e un Bloom Filter √† l'aide de la commande "BF.RESERVE" nomm√©e (blackListIP) avec une probabilit√© d'erreur de 0.0001 et une taille de 100 √©l√©ments.</p>
<p>
Ensuite, trois adresses IP ont √©t√© ajout√©es au filtre √† l'aide de la commande "BF.MADD". Les r√©ponses "1" indiquent que les trois ajouts ont r√©ussi, ce qui signifie que ces adresses IP sont maintenant consid√©r√©es comme pr√©sentes dans le filtre.
<p>
Pour v√©rifier si une adresse IP particuli√®re est dans la liste noire, nous avons utilis√© la commande "BF.MEXISTS". Lorsque l'adresse IP (89.30.99.12) a √©t√© test√©e, la r√©ponse a indiqu√© "0", ce qui signifie que cette adresse n'appartient pas √† la liste noire. En revanche, pour l'adresse IP (86.10.123.16), la r√©ponse a √©t√© 1, ce qui signifie que cette adresse fait partie de la liste noire.
</p>
<p>
En r√©sum√©, l'utilisation du Bloom Filter ici permet de v√©rifier rapidement et de mani√®re probabiliste si une adresse IP donn√©e est pr√©sente dans une liste noire, ce qui est utile pour d√©tecter et bloquer des adresses IP ind√©sirables.
</p>


<center>
<img
  src="images/image5.png"
  alt="V√©rification des adresses IP dans une liste noire avec Bloom Filters"
  width="400"
  height="350" 
  style="text-align: center"/>
<h5><b>Figure 5: V√©rification des adresses IP dans une liste noire avec Bloom Filters<b></h5>
</center>

<h3>5. √âtude des performances de Redis:</h3>

<p>
Dans le cadre de cette √©tude, notre objectif est d'analyser la performance de Redis avec deux m√©thodes differentes, √† savoir les filtrers de Bloom et les listes, dans le contexte de la gestion de grandes quantit√©s de donn√©es. Dans cette √©tude on va utiliser le dictionnaire disponible sur le lien suivant: <a href="rali.iro.umontreal.ca/DEM//DEM-1_1.csv">Cliquez ici.</a></p>
<p>
La probl√©matique est: Comment les filtres de Bloom et les listes se comportenet-ils lors du stockage et de la r√©cuperation de 100 000 mots ? Pour r√©pondre √† cette question, nous mettrons en oeuvre un test afin de mesurer les temps d'insertion et d'appartenance des mots pour chaque structure de donn√©es.
</p>
<p>
Dans cette d√©marche comparative, nous cherchons √† d√©terminer laquelle des m√©thodes offre la meilleure performance en termes de temps d'acc√®s et de r√©ponses.
</p>

***Premi√®re √©tape: Pr√©paration du travail***

<p>Avant de commmencer on va convertir le fichier CSV vers un format JSON en mettant particuli√®rement l'accent sur deux cl√©s essentielles: mot "M" et d√©finition "SENS". Le choix du format JSON a √©t√© par sa capacit√© √† repr√©senter des informations complexes et √† sa flexibilit√©.



```python
import pandas as pd
import json

# Charger le fichier CSV et S√©lectionner juste les deux colonnes
df = pd.read_csv("csv/test.csv", delimiter = '\t', usecols=['M', 'SENS'])

# Convertir le DataFrame s√©lectionn√© en format JSON
json_data = df.to_json(orient='records', force_ascii=False)

# √âcrire le fichier JSON
json_file_path = 'json/my_json.json'
with open(json_file_path, 'w') as json_file:
    json_file.write(json_data)

# V√©rifier la transformation
with open('json/my_json.json', 'r', encoding='utf-8') as file:
    data = json.load(file)
#for item in data[:3]:
    #print(item.get('M'))
    #print(item.get('SENS'))
```

***Deuxi√®me √©tape: Gestion des √©lements nuls***

<p>
Apr√®s avoir accompli la premi√®re √©tape (la conversion du fichier CSV vers un fichier JSON), une observation cruciale a √©t√© faite, c'est que le fichier contient des √©l√©ments nuls  qui peuveut entrainer des probl√®mes lors de l'insertion des donn√©es dans redis que ce soit la mani√®re utiliser (Bloom filter ou liste). C'est pour √ßa on va impl√©menter une m√©thode qui permet de filter les √©l√©ments nuls.
</p>


```python
def to_list(iterable):
    return [element for element in iterable if element is not None]
```


```python
df = pd.read_json('json/my_json.json')
mots_definitions = to_list(df['M'].tolist())
definitions = to_list(df['SENS'].tolist())

#Jointure
mots_definitions.extend(definitions)
```

***Trois√®me √©tape: √âtude de la performance avec Bloom filter:***


```python
import redis
import json
import time

client_redis = redis.Redis(host='localhost', port=6379)
```


```python
#M√©thode d'insertion avec bloom filter
def insert_bloomfilter(list):    
    BLOOM_FILTER_KEY = "words:bloom_filter"
    client_redis.bf().reserve(BLOOM_FILTER_KEY, 0.01, len(list))
    start_time = time.time()
    client_redis.bf().madd(BLOOM_FILTER_KEY, *list)
    elapsed_time = time.time() - start_time
    return print(f"Temps d'insertion pour bloom filter est: {elapsed_time} secondes")

#Test
insert_bloomfilter(mots_definitions)
```

    Temps d'insertion pour bloom filter est: 1.1293649673461914 secondes



```python
#M√©thode de recherche avec bloom filter
def search_bloomfilter(redis_client, list, bloomfilter_key):
    start_time = time.time()
    
    result = redis_client.bf().mexists(bloomfilter_key, *element)
    elapsed_time = time.time() - start_time
    print(f"Temps d'appartenance avec bloom filter est : {elapsed_time} secondes")

my_bloom_filter= "words:bloom_filter"
value_to_search = mots_definitions[len(mots_definitions)-1]

#Test
search_bloomfilter(client_redis, mots_definitions, list_key)
```

    Temps pour bloom filter : 0.003640890121459961 secondes


***Quatri√®me √©tape: √âtude de la performance avec les listes:***


```python
#M√©thode d'insertion avec les listes
def insert_list(list):    
    LIST_KEY = "words:list"
    start_time = time.time()
    client_redis.rpush(LIST_KEY, *list)
    elapsed_time = time.time() - start_time
    return print(f"Temps d'insertion pour une liste : {elapsed_time} secondes")
    
#Test
insert_list(mots_definitions)
```

    Temps d'insertion pour une liste : 0.8318448066711426 secondes



```python
#M√©thode de recherche avec les listes
def search_list(redis_client, list, list_key):
    start_time = time.time()
    for element in list:
        position = redis_client.lpos(list_key, element)
    elapsed_time = time.time() - start_time
    print(f"Temps d'appartenance avec une liste est: {elapsed_time} secondes")
    average_time_per_membership = elapsed_time / len(list)
    print(f"Temps de chaque mot est: {average_time_per_membership} secondes")
    
list_key= "words:list"

#Test
search_list(client_redis, mots_definitions[:10000], list_key)
```

    Temps pour list : 11.410327196121216 secondes
    Temps/word pour list : 0.0011410327196121216 secondes


***Remarque***
<p>
Suite √† l'insertion des donn√©es dans Redis, nous avons not√© une observation importante concernant l'espace occup√© par chacune. Cette observation est pr√©sent√©e dans <b>la figure 6</b>, le filtre de Bloom occupe un espace m√©moire de 391 kilo-octets (KB). Cette mesure refl√®te la taille n√©cessaire pour stocker les √©l√©ments dans le filtre de Bloom. Parall√®lement, la liste utilis√©e dans notre √©tude occupe 4 m√©gaoctets (MB) d'espace m√©moire.
</p>
<p>
La diff√©rence significative entre l'espace m√©moire occup√© par le filtre de Bloom et celui occup√© par la liste indique des caract√©ristiques distinctes en termes d'efficacit√© de stockage. Le filtre de Bloom semble √™tre une option plus √©conomique en termes d'espace m√©moire, occupant sensiblement moins d'espace que la liste traditionnelle.
</p>
<center>
<img
  src="images/image6.png"
  alt="Comparaison de l'espace m√©moire occup√©"
  width="500"
  height="450" 
  style="text-align: center"/>
<h5><b>Figure 6: Comparaison de l'espace m√©moire occup√©<b></h5>
</center>

<h4>Conclusion:</h4>
<p>
Les r√©sultats de nos fonctions de performance montrent clairement que l'utilisation de Bloom filter offre des performances sup√©rieures pour l'op√©ration de la recherche. L'insertion avec Bloom filter plus lente que celle avec la liste, mais la diff√©rene est minime. En revanche, la recherche avec Bloom filter est extremement rapide par rapport √† la liste, ce qui montre l'efficacit√© de Bloom filter pour les op√©ration d'appartenance.
</p>
<p>
Finalement, l'utilisation de Bloom filter se r√©v√®le avantageuse pour les op√©rations de recherche, offrant aussi une solution performante et √©conome de temps d'ex√©cution.
</p>

<h3>5. Publication et souscription</h3>
<p>
    Le principe de publication et de souscription est un mod√®le de communication entre diff√©rents composants logiciels ou syst√®mes.
Il permet √† des parties distinctes de s'√©changer des informations de mani√®re asynchrone, sans qu'elles soient directement coupl√©es les unes aux autres. Dans ce contexte, la situation d√©crite est semblage √† ce qui est repr√©sent√© dans <b> la figure 7</b>. 
   
<center>
<img
  src="images/image7.png"
  alt="Publication et souscription"
  width="600"
  height="350" 
  style="text-align: center ;border:1px solid black""/>
<h5><b>Figure 7: Publication et souscription<b></h5>
</center>
</p>
<h4>5.1. Sc√©nario d'application: calcul de la moyenne de CO2:</h4>
<p>
L'objectif de cette application est de mettre en place le m√©canisme de pub/sub en utilisant deux script. Le premier agit en tant que producteur qui publie de mani√®re p√©riodique des mesures de CO2 en utilisant le canal "co2_channel", qui sont g√©n√©r√©es al√©atoirement, le script s'ex√©cute d'une mani√®re continue et al√©atoire avec un intervalle entre 1 et 3 secondes.
</p>
<p>
Le deuxi√®me script fonctionne comme un recepteur, qui recoit les donn√©es. √Ä chaque minute il r√©cup√®re les donn√©es du canal et calcule la moyenne des valeurs de CO2 re√ßues au cours de la d√©rni√®re minute, des d√©rni√®res 30 minutes et la d√©rni√®re heure. Le scipt utilise le canal "co2_channel" pour souscrir aux donn√©es publi√©es par le premier script.
</p>
<p>Si vous utilisez jupyter, vous n'avez pas la capacit√© √† ex√©cuter les deux scripts en parall√®le. Pour r√©soudre ce probl√®me, de vous invite √† voir <b>l'Annexe 4</b>.</p>

***Premi√®re √©tape: Publication des Donn√©s:***


```python
import json
import redis
import random
from datetime import datetime
import time

r = redis.Redis(host='localhost', port=6379) 

def generate_random_co2_value():
    return round(random.uniform(300, 1000))

def send_co2():
    current_date = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
    co2_value = generate_random_co2_value()
    
    # Cr√©er un dictionnaire
    data = {'date': current_date, 'co2_value': co2_value}
    
    # Convertir le dictionnaire en format JSON
    json_data = json.dumps(data)
    
    r.publish('co2_channel', json_data)

while True:
    random_seconds = random.uniform(1, 3)
    send_co2()
    time.sleep(random_seconds)

```

***Deuxi√®me √©tape √©tape: Souscription des Donn√©s:***


```python
import redis
import json 
import time
from datetime import datetime, timedelta
import schedule


r = redis.Redis(host='localhost', port=6379) 
pubsub = r.pubsub()
channel = 'co2_channel'
pubsub.subscribe(channel)
historique=[]

def get_data():
    message = pubsub.get_message()
    if(message and message.get('data') and message.get('data') != 1):
        data = json.loads(message.get('data'))
        msg = {"date": datetime.strptime(data['date'], '%Y-%m-%d %H:%M:%S'), "co2_value": data.get("co2_value")}
        historique.insert(0, msg)

def get_last_n_minutes(minutes, historique):
    now = datetime.now()
    time_threshold = now - timedelta(minutes=minutes)
    return [item for item in historique if item['date'] >= time_threshold]

def calculate_avg(historique):
    co2_values = [item['co2_value'] for item in historique]
    return sum(co2_values) / len(co2_values)

def calculate():
    if(len(historique) > 0):
        last_1_minutes = get_last_n_minutes(1, historique)
        avg_last_1_minutes = calculate_avg(last_1_minutes)

        last_30_minutes = get_last_n_minutes(30, historique)
        avg_last_30_minutes = calculate_avg(last_30_minutes)

        last_1_hour = get_last_n_minutes(60, historique)
        avg_last_1_hour = calculate_avg(last_1_hour)

        print("Derni√®re minute: " + str(round(avg_last_1_minutes, 1)))
        print("Derni√®res 30 minutes: " + str(round(avg_last_30_minutes, 1)))
        print("Derni√®re heure: " + str(round(avg_last_1_hour, 1)))
        print("--------------------------")
    

schedule.every(1).minutes.do(calculate)

while True:
    get_data()
    schedule.run_pending()
    time.sleep(1)
```

<h4>Conclusion:</h4>
<p>
En conclusion, les r√©sultats obtenus d√©montrent l'efficacit√© du m√©canisme pub/sub mis en place. Le script de souscription (subscribe) r√©ussit √† recevoir de mani√®re fiable les mesures de CO2 publi√©es par le script de publication (publish) sur le canal "co2_channel". 
</p>
<p>
La flexibilit√© du canal "co2_channel" se r√©v√®le particuli√®rement avantageuse, car il peut √™tre utilis√© avec plusieurs instances des scripts de mani√®re simultan√©e.

<h1>III. D√©couverte de la base de donn√©e MongoDB:</h1>
<p>
MongoDB est une base de donn√©es orient√©e documents. En clair, vous b√©n√©ficiez de la scalabilit√© et de la flexibilit√© que vous voulez, avec les fonctions d‚Äôinterrogation et d‚Äôindexation qu‚Äôil vous faut.
Alors, √™tes-vous pr√™t √† plonger dans l'aventure MongoDB ? Dans un instant, nous allons mettre en pratique ces concepts et ma√Ætriser l'art de la gestion de donn√©es avec agilit√©. Attachez vos ceintures, c'est parti ! üöÄ
</p>

<h3>1. Installation:</h3>
<p>
La proc√©dure d√©taill√©e est disponible dans <b>l'Annexe 3</b>. Pour une installation efficace de MongoDB, je vous recommande d'utiliser Docker Compose et MongoDB Compass pour la visualisation. Cette approche simplifi√©e vous permettra de d√©ployer MongoDB dans des conteneurs Docker tout en facilitant la gestion avec Docker Compose.
</p>

<h3>2. Commandes fondamentales  de MongoDB:</h3>
<p>
<p>Avant de commencer, il est essentiel de connaitre le format de stockage de MongoDB. Les informations sont stock√©es dans des documents au format JSON (plus exactement BSON, une version binaire du JSON).
<p>
</p>
<H4> 2.1. Cr√©ation de collection:</H4>
<p>   
MongoDB est structur√© autour de 3 √©l√©ments : <b> le document</b>, <b>la collection</b> et<b> la database.</b>

    - Une database contient une ou plusieurs collections.
    - Une collection regroupe un ensemble de documents.
</p>


```python
pip install pymongo
```

    Requirement already satisfied: pymongo in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (4.6.0)
    Requirement already satisfied: dnspython<3.0.0,>=1.16.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from pymongo) (2.4.2)
    
    [1m[[0m[34;49mnotice[0m[1;39;49m][0m[39;49m A new release of pip is available: [0m[31;49m23.3.1[0m[39;49m -> [0m[32;49m23.3.2[0m
    [1m[[0m[34;49mnotice[0m[1;39;49m][0m[39;49m To update, run: [0m[32;49mpip install --upgrade pip[0m
    Note: you may need to restart the kernel to use updated packages.



```python
import pymongo
from pymongo import MongoClient

clientDB = pymongo.MongoClient("<db_userName>:<db_password>@<db_url>/")
mydb = clientDB["BUT3"]
mycollection = mydb["Informatique"]  
```

<H4> 2.2. M√©thodes d'insertion:</H4>

<H5> 2.2.1. Insertion d'un objet JSON:</H5>


```python
mycollection.insert_one({ "name": "ikram", "address": "paris" })

for y in mycollection.find():
  print(y) 
```

    {'_id': ObjectId('6585421f43341b608c85fccb'), 'name': 'ikram', 'address': 'paris'}
    {'_id': ObjectId('6585422643341b608c85fccc'), 'name': 'ikram', 'address': 'paris'}


<p>
    <b>La figure 8</b> montre une d√©monstration d'insertion d'un objet JSON dans MongoDB, "BUT3" repr√©sente le nom de la base de donn√©es, tandis que "INFORMATIQUE" est le nom de la collection au sein de cette base de donn√©es. L'op√©ration d'insertion concerne un unique document, sous la forme d'un objet JSON, qui est ajout√© √† la collection sp√©cifi√©e. Il est not√© que cet objet est ins√©r√© avec un index par d√©faut, sugg√©rant probablement l'utilisation d'un index automatique assign√© par MongoDB lors de l'insertion si aucun index explicite n'est sp√©cifi√©.
</p>

<center>
<img
  src="images/image8.png"
  alt="D√©monstration d'insertion des donn√©e dans MongoDB"
  width="700"
  height="550" 
  style="text-align: center ;border:1px solid black"/>
<h5><b>Figure 8: D√©monstration d'insertion des donn√©e dans MongoDB<b></h5>  
</center>

<H5> 2.2.2. Insertion d'un tableau d'objet:</H5>


```python
import json

mydb = clientDB["Test_json"]
mycollection = mydb["json"]

with open("json/json1.json") as j1:
    json1 = json.load(j1)
    mycollection.insert_many(json1)  
    
res= mycollection.find()
for element in res:
    print(element)
```

    {'_id': ObjectId('658542a243341b608c85fcd5'), 'cle': '001', 'lieu': 'Mar 1'}
    {'_id': ObjectId('658542a243341b608c85fcd6'), 'cle': '001', 'lieu': 'Rane 1'}
    {'_id': ObjectId('658542a243341b608c85fcd7'), 'cle': '002', 'lieu': 'Mar 2'}
    {'_id': ObjectId('658542a243341b608c85fcd8'), 'cle': '002', 'lieu': 'Rane 2'}
    {'_id': ObjectId('658542a243341b608c85fcd9'), 'cle': '003', 'lieu': 'Mar 3'}
    {'_id': ObjectId('658542a243341b608c85fcda'), 'cle': '003', 'lieu': 'Rane 3'}
    {'_id': ObjectId('658542a243341b608c85fcdb'), 'cle': '003', 'lieu': 'Lila 1'}
    {'_id': ObjectId('658542a243341b608c85fcdc'), 'cle': '004', 'lieu': 'Mar 4'}


<p>
<b>La figure 8</b> repr√©sente le flux de travail de notre script d'insertion d'un tableau d'objets. Chaque objet JSON est stock√© sous forme de document dans la collection MongoDB et chaque document poss√®de un identifiant unique. Si aucun identifiant n'est sp√©cifi√© pour un document, MongoDB attribue automatiquement un ID unique √† ce document.

Cette approche assure une int√©gration efficace des donn√©es JSON dans MongoDB, offrant une flexibilit√© et une granularit√© dans la manipulation des documents au sein de la collection "Test_json". 
</p>

<center>
<img
  src="images/image9.png"
  alt="D√©monstration de l'insertion de fichier JSON dans MongoDB"
  width="700"
  height="550" 
  style="text-align: center ;border:1px solid black"/>
<h5><b>Figure 9: D√©monstration de l'insertion de fichier JSON dans MongoDB<b></h5>  
</center>

<H4> 2.3. R√©cuperer le nombre de documents d'une collection:</H4>


```python
json_count = mycollection.count_documents({})
print(json_count)
```

    8


<H4> 2.4. Supprimer le contenu d'une collection:</H4>


```python
delete= mycollection.delete_many({})
```

<h3>3. La jointure des Objets JSON avec MongoDB:</h3>
<p>
MongoDB, offre une solution flexible pour effectuer des op√©ration de jointure √† l'aide de son pipeline d'agr√©gation, qui permet de fusionner des donn√©es provenant de diff√©rentes collections.
</p>
<p>
Tout d'abord, il faut utiliser <b>"lookup"</b> qui permet de sp√©cifier la collection externe √† joindre et les champs et les champs qui serviront de cl√©s de jointure. <b>"project"</b> pour s√©lectionner les champs √† inclure dans le r√©sultat final. 
</p>


```python
from pymongo import MongoClient
import json
import pymongo

clientDB = pymongo.MongoClient("mongodb://<db_userName>:<db_password>@<db_url>/")
mydb = clientDB["test_jointure"]
collection_json1 = mydb["json1"]  
collection_json2 = mydb["json2"] 

with open("json/json1.json") as j1:
    json1 = json.load(j1)
    collection_json1.insert_many(json1)  
with open("json/json2.json") as j2:
    json2 = json.load(j2)
    collection_json2.insert_many(json2)  
```


```python
resultat = collection_json1.aggregate([
    {
        "$lookup": {
            "from": "json2",
            "localField": "cle",
            "foreignField": "cle",
            "as": "json2"
        }
    },
    {
        "$unwind": "$json2"
    },
    {
        "$project": {
            "cle": 1,
            "lieu": 1,
            "info": "$json2.info"
        }
    }
])
collection_json3 = mydb["json_resultat"] 
collection_json3.insert_many(resultat)

res= collection_json3.find()
for element in res:
    print(element)
```

    {'_id': ObjectId('658542f143341b608c85fcf8'), 'cle': '001', 'lieu': 'Mar 1', 'info': "L'info 1"}
    {'_id': ObjectId('658542f143341b608c85fcf9'), 'cle': '001', 'lieu': 'Rane 1', 'info': "L'info 1"}
    {'_id': ObjectId('658542f143341b608c85fcfa'), 'cle': '002', 'lieu': 'Mar 2', 'info': "L'info 2"}
    {'_id': ObjectId('658542f143341b608c85fcfb'), 'cle': '002', 'lieu': 'Rane 2', 'info': "L'info 2"}
    {'_id': ObjectId('658542f143341b608c85fcfc'), 'cle': '003', 'lieu': 'Mar 3', 'info': "L'info 3"}
    {'_id': ObjectId('658542f143341b608c85fcfd'), 'cle': '003', 'lieu': 'Rane 3', 'info': "L'info 3"}
    {'_id': ObjectId('658542f143341b608c85fcfe'), 'cle': '003', 'lieu': 'Lila 1', 'info': "L'info 3"}
    {'_id': ObjectId('658542f143341b608c85fcff'), 'cle': '004', 'lieu': 'Mar 4', 'info': "L'info 4"}


<h3>4. √âtude des performances de MongoDB:</h3>

<p>Notre objectif est d'analyser l'impact des index sur la performance pour la r√©cup√©ration de grandes quantit√©s de donn√©es. Nous comparerons les performances de MongoDB avec index et sans index en utilisant le m√™me dictionnaire que celui employ√© pr√©c√©demment avec Redis: <a href="rali.iro.umontreal.ca/DEM//DEM-1_1.csv">Cliquez ici.</a>.
</p>
<p>
La probl√©matique centrale de notre √©tude est la suivante : Comment les index influencent-ils la performance pour la r√©cup√©ration de 100 000 mots dans le contexte de MongoDB ? Afin de r√©pondre √† cette question, nous allons mettre en ≈ìuvre un test approfondi visant √† mesurer les temps d'appartenance des mots pour chaque configuration, avec et sans index.
</p>
<p>
Dans cette d√©marche comparative, notre objectif est de d√©terminer laquelle des m√©thodes offre la meilleure performance en termes de temps d'acc√®s et de r√©ponses.
</p>
<p>
En utilisant le m√™me jeu de donn√©es que celui utilis√© dans l'√©tude pr√©c√©dente avec Redis, nous pourrons comparer directement les r√©sultats obtenus entre les deux bases de donn√©es. Cette analyse comparative nous permettra de prendre des d√©cisions √©clair√©es quant √† l'utilisation ou non d'index dans des sc√©narios similaires avec MongoDB, contribuant ainsi √† optimiser la gestion de grandes quantit√©s de donn√©es.</p>
</p>


***Premi√®re √©tape: √âtude de la performance sans index :***


```python
import json 
from pymongo import MongoClient

clientDB = MongoClient("mongodb://<db_userName>:<db_password>@<db_url>/")
mydb_one = clientDB["dictionnaire"]
mycollection_one = mydb_one["collection_without_index"] 

with open("json/my_json.json") as file:
        data = json.load(file)
    
res=mycollection_one.insert_many(data)

def search_by_name_without_index(word):
    temps_debut=time.time()
    element= mycollection_one.find({"M": word},{"M":1})
    for elm in element:
        print(elm)
    temps_fin=time.time()
    print("temps de recherche est: "+str(temps_fin-temps_debut)+" secondes")

search_by_name_without_index("√† la rencontre de")

```

    {'_id': ObjectId('6585431143341b608c85ff0d'), 'M': '√† la rencontre de'}
    temps de recherche est: 0.10278797149658203 secondes


***Deuxi√®me √©tape: √âtude de la performance avec index :***


```python
clientDB = MongoClient("mongodb://<db_userName>:<db_password>@<db_url>/")
mydb_two = clientDB["dictionnaire"]
mycollection_two = mydb_two["collection_with_index"] 

with open("json/my_json.json") as file:
        data1 = json.load(file)
    
res1=mycollection_two.insert_many(data1)

#Cr√©ation d'index
index = [("M",1)]
mycollection_two.create_index(index)

def search_by_name_with_index(word):
    temps_debut=time.time()
    element= mycollection_two.find({"M": word},{"M":1})
    for elm in element:
        print(elm)
    temps_fin=time.time()
    print("temps de recherche est: "+str(temps_fin-temps_debut)+" secondes")

search_by_name_with_index("√† la rencontre de")

```

    {'_id': ObjectId('6585431643341b608c88363b'), 'M': '√† la rencontre de'}
    {'_id': ObjectId('6585432543341b608c8a6d69'), 'M': '√† la rencontre de'}
    temps de recherche est: 0.001825094223022461 secondes


<h4>R√©sultat:</h4>
<p>
Les r√©sultats obtenus mettent en √©vidence une distinction notable dans les performances entre l'utilisation d'index et l'absence d'index. Lorsque les index sont utilis√©s, le temps n√©cessaire pour v√©rifier l'appartenance d'une donn√©e est extremement court. En revanche, sans l'utilisation d'index, le temps augmente consid√©rablement.
</p>
<p>
Notre √©tude d√©montre clairement l'impact positif des index sur la performance des op√©rations d'appartenance de donn√©es dans MongoDB. Les r√©sultats montrent une r√©duction significative du temps d'acc√®s, passant de ‚âà 0.080 secondes sans index √† seulement ‚âà 0.002 secondes avec index. Ces conclusions soulignent l'importance strat√©gique de l'utilisation d'index pour optimiser les temps de recherche, offrant ainsi des pistes concr√®tes pour am√©liorer l'efficacit√© op√©rationnelle de notre syst√®me de gestion de donn√©es.
</p>

<p>
Pour obtenir une vision plus clair des performances entre une recherche avec et sans index, nous nous appuierons sur l'outil MongoDB Compass. Dans <b>la Figure 10</b>, nous observons une recherche sans index, indiquant que le moteur de recherche de MongoDB a parcouru la totalit√© des 145 197 documents pour trouver le r√©sultat requis, avec un temps d'ex√©cution de 118 ms. En contraste, <b>la Figure 11</b>, illustrant la recherche avec index, r√©v√®le une am√©lioration significative. Gr√¢ce √† l'utilisation de l'index 'M', le moteur de recherche de MongoDB a consult√© uniquement les documents pertinents, permettant ainsi une recherche d'une rapidit√© remarquable de 0 ms.
</p>

<center>
<img
  src="images/insertion_sans_index.png"
  alt="D√©monstration de l'insertion de fichier JSON dans MongoDB"
  width="700"
  height="550" 
  style="text-align: center ;border:1px solid black"/>
<h5><b>Figure 10: D√©tails de la recherche sans index <b></h5>  
</center>
    <br>
    <br>
<center>
<img
  src="images/insertion_index.png"
  alt="D√©monstration de l'insertion de fichier JSON dans MongoDB"
  width="700"
  height="550" 
  style="text-align: center ;border:1px solid black"/>
<h5><b>Figure 11: D√©tails de la recherche avec index  <b></h5>  
</center>

<h4> Conclusion:</h4>
<div style="text-align: justify">
<p>
Pour conclure, notre pr√©sente √©tude sur MongoDB, en compl√©ment de l'√©tude pr√©c√©dente sur Redis, s'est concentr√©e sur la comparaison du temps d'appartenance entre les technologies Redis avec Bloom Filter, MongoDB sans Index, et MongoDB avec Index.
</p>
<p>
Les r√©sultats indiquent que Redis avec Bloom filter excelle dans la rapidit√© des op√©rations d'appartenance. Cependant, la gestion prudente de la consommation de m√©moire est n√©cessaire.
</p>
<p>
D'un autre c√¥t√©, MongoDB avec index a d√©montr√© une am√©lioration notable des performances, particuli√®rement b√©n√©fique pour des requ√™tes complexes, bien que cette optimisation s'accompagne d'une augmentation de l'utilisation de l'espace disque.
</p>
<p>
MongoDB sans index, pr√©sente des limitations √©videntes lorsque la taille de la base de donn√©es augmente, comme notre √ßa d‚Äô√©tude, mais cette m√©thode viable pour des bases de donn√©es plus petites ou des op√©rations simples.
</p>
<p>
Finalement, le choix entre ces solutions d√©pendra des besoins sp√©cifiques de l'application, mettant en balance la rapidit√©, la consommation de m√©moire et l'utilisation de l'espace disque.
</p>
</div>
<center>
<img
  src="images/redis_mogodb.jpg"
  alt="Redis_vs_mongodb"
  width="700"
  height="550" 
  style="text-align: center ;border:1px solid black"/>
</center>

<h3>5. Publication et souscription:</h3>
<p>
Contrairement √† Redis, MongoDB ne prend pas en charge nativement le mod√®le pub/sub. Mais malg√© cette absence, il existe des solutions √† developper par nous les d√©veloppeurs pour attreindre des fonctionnalit√©s similaires.
</p>
<b>Solution mise en oeuvre avec MongoDB:</b>
<p>
Change Streams: C'est une fonctionnalit√© MongoDB qui permet de suivre les changements au niveau de la base de donn√©es en temps r√©el, alors, ils sont utilis√©s pour d√©tecter les changements dans une collection et d√©clencher des actions en cons√©quences. Cependant, pour activer les Change Streams, il est imp√©ratif d'activer le syst√®me r√©plicaset de MongoDB. Je vous invite √† consulter<a href="https://www.mongodb.com/docs/manual/changeStreams/"> le lien suivant </a>pour plus des informations.
</p>



<p>Si vous utilisez jupyter, vous n'avez pas la capacit√© √† ex√©cuter les deux scripts en parall√®le. Pour r√©soudre ce probl√®me, de vous invite √† voir <b>l'Annexe 4</b>.</p>


```python
#Test publisher:
```

<b>publisher_mongodb.py<b/>


```python
from pymongo.mongo_client import MongoClient
from pymongo.server_api import ServerApi
from datetime import datetime

import random
import time
import certifi

# Create a new client and connect to the server
def connectToMongodb():
    uri="mongodb+srv://<db_name>:<db_password>@<db_url>/"
    client = MongoClient(uri, server_api=ServerApi('1'), tlsCAFile=certifi.where())
    try:
        client.admin.command('ping')
        print("Pinged your deployment. You successfully connected to MongoDB!")
    except Exception as e:
        print(e)
    return client

# Connexion √† la base de donn√©es
client = connectToMongodb()
collection = client["mydatabase"]['co2_channel']

def generate_random_co2_value():
    return round(random.uniform(300, 1000))

def send_co2():
    current_date = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
    co2_value = generate_random_co2_value()
    
    # Cr√©er un dictionnaire
    data = {'date': current_date, 'co2_value': co2_value}

    #Insertion dans le dictionnaire  
    collection.insert_one(data)

while True:
    random_seconds = random.uniform(1, 3)
    send_co2()
    time.sleep(random_seconds)

```


```python
#Test subscriber:
```

<b>subscriber_monodb.py</b>


```python
from datetime import datetime, timedelta
from pymongo.mongo_client import MongoClient
from pymongo.server_api import ServerApi
from datetime import datetime

import schedule
import certifi

def connectToMongodb():
    uri="mongodb+srv://<db_name>:<db_password>@<db_url>/"
    client = MongoClient(uri, server_api=ServerApi('1'), tlsCAFile=certifi.where())
    try:
        client.admin.command('ping')
        print("Pinged your deployment. You successfully connected to MongoDB!")
    except Exception as e:
        print(e)
    return client

# Connexion √† la base de donn√©es
client = connectToMongodb()
collection = client["mydatabase"]['co2_channel']

# Tableau qui va stocker les donn√©es re√ßu du publisher
historique=[]

# Recup√©ration des donn√©es de MongoDB (document_id: id du document inser√©)
def get_data(document_id):
    message = collection.find_one({"_id": document_id})
    if(message):
        data = message
        msg = {"date": datetime.strptime(data['date'], '%Y-%m-%d %H:%M:%S'), "co2_value": data.get("co2_value")}
        historique.insert(0, msg)

def get_last_n_minutes(minutes, historique):
    now = datetime.now()
    time_threshold = now - timedelta(minutes=minutes)
    return [item for item in historique if item['date'] >= time_threshold]

def calculate_avg(historique):
    co2_values = [item['co2_value'] for item in historique]
    return sum(co2_values) / len(co2_values)

def calculate():
    if(len(historique) > 0):
        last_1_minutes = get_last_n_minutes(1, historique)
        avg_last_1_minutes = calculate_avg(last_1_minutes)

        last_30_minutes = get_last_n_minutes(30, historique)
        avg_last_30_minutes = calculate_avg(last_30_minutes)

        last_1_hour = get_last_n_minutes(60, historique)
        avg_last_1_hour = calculate_avg(last_1_hour)

        print("Derni√®re minute: " + str(round(avg_last_1_minutes, 1)))
        print("Derni√®res 30 minutes: " + str(round(avg_last_30_minutes, 1)))
        print("Derni√®re heure: " + str(round(avg_last_1_hour, 1)))
        print("--------------------------")

schedule.every(1).second.do(calculate)

# Cr√©er un Change Stream sur la collection qui ecoute que les op√©ration de type insert
change_stream = collection.watch([{"$match": {"operationType": "insert"}}])

# G√©rer les √©v√©nements de changement
for change in change_stream:
    document_id = change["documentKey"]["_id"]
    get_data(document_id)
    #Pour lancer le schedule
    schedule.run_pending()

```

<h4>Conclusion:</h4>
<p>
En conclusion, l'exploration des bases de donn√©es NoSQL, en particulier MongoDB et Redis, a r√©v√©l√© des caract√©ristiques distinctes et des performances sp√©cifiques. MongoDB, en tant que syst√®me de gestion de base de donn√©es orient√© document, a montr√© une efficacit√© notable pour les op√©rations de lecture et d'√©criture complexes, avec une flexibilit√© accrue dans la mod√©lisation des donn√©es. D'autre part, Redis, en mettant en ≈ìuvre des structures de donn√©es en m√©moire, a d√©montr√© des performances exceptionnelles pour les op√©rations de lecture rapide, en tirant parti de structures telles que les bloom filters pour am√©liorer l'efficacit√© des op√©rations d'appartenance.
</p>
</p>
<p>
L'analyse comparative des performances avec et sans index dans MongoDB a r√©v√©l√© l'impact crucial des indices sur l'efficacit√© des requ√™tes. Cette constatation souligne l'importance de la conception r√©fl√©chie de l'indexation pour optimiser les performances des requ√™tes dans un environnement MongoDB.
</p>
<p>
Par ailleurs, l'√©tude de la fonctionnalit√© pub/sub dans Redis a mis en √©vidence la capacit√© de ce syst√®me √† fournir une communication asynchrone efficace entre les diff√©rentes parties de l'application, offrant ainsi une flexibilit√© pr√©cieuse dans le traitement des donn√©es.
</p>
<p>
En somme, l'utilisation judicieuse de ces bases de donn√©es NoSQL en fonction des exigences sp√©cifiques de chaque application est cruciale. MongoDB et Redis offrent des avantages distincts, et le choix entre eux d√©pendra des besoins pr√©cis en termes de performances, de structure des donn√©es et de fonctionnalit√©s requises. Cette √©tude approfondie fournit des insights pr√©cieux pour vous guidez dans la prise de d√©cision concernant le choix de la base de donn√©es NoSQL la mieux adapt√©e √† vos projets.
</p>

<h2>Table des Annexes:</h2>
<h4>Annexes 1: Installation de Redis</h4>

- Cr√©ez un fichier `Dockerfile` avec la configuration.<br>

<center>
<img
  src="images/image10.png"
  alt="D√©ploiement de Redis avec Dokerfile"
  width="400"
  height="350" 
  style="text-align: center"/>
<h5><b>Annexes 1: D√©ploiement de Redis avec Dokerfile<b></h5>  
</center>
    
<h4>Annexes 2:  Installation de Redis (suite)</h4>

- Cr√©ez un fichier `DockerCompose.yml` avec la configuration.<br>
<p>
   
</p>
<center>
<img
  src="images/image11.png"
  alt="Orchestration de d√©ploiement de Redis"
  width="400"
  height="350" 
  style="text-align: center"/>
<h5><b>Annexes 2: Orchestration de d√©ploiement de Redis<b></h5>
</center>
    
<h4>Annexes 3: Installation de MongoDB</h4>
<p>
    
1. Installation de MongoDB avec Docker Compose :
    - Cr√©ez un fichier `docker-compose.yml`avec la configuration.<br>
    - Ex√©cutez la commande `docker-compose up -d` pour d√©marrer MongoDB avec Docker Compose.<br>
    
<center>
<img
  src="images/image12.png"
  alt="Installation de MongoDB"
  width="400"
  height="350" 
  style="text-align: center"/>
<h5><b>Annexes 3: Installation de MongoDB<b></h5>
</center>
    
2. Utilisation de MongoDB Compass pour la visualisation :
   - Installez MongoDB Compass depuis le site officiel <a href="https://www.mongodb.com/products/tools/compass">Cliquez ICI.</a> 
   - Lancez MongoDB Compass et connectez-vous √† votre instance MongoDB avec l'adresse `localhost:27017` et les identifiants du fichier `docker-compose.yml`.

</p>



<h4>Annexes 4: √âxecution des scripts Pub/Sub </h4>
<p>
    
1. Positionnement dans le r√©peroire:
- Ouvrez un terminal ou une console dans le r√©pertoire o√π se trouvent les scripts<br>
- Ex√©cutez la commande `$cd Path/repertoire` .<br>
    
2. Lancement du Script Subscriber :
- Ex√©cutez la commande `$python3 publisher_mongodb.py` .<br> 
  
3. Lancement du Script Publisher :
- Dans un autre terminal, ex√©cutez la commande `$python3 subscriber_monodb.py` .<br> 
</p>
